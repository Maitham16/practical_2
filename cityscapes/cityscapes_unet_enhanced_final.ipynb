{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_dir = '/home/maith/Desktop/practical_2/cityscapes'\n",
    "train_images_dir = os.path.join(data_dir, 'leftImg8bit/train')\n",
    "train_labels_dir = os.path.join(data_dir, 'gtFine/train')\n",
    "\n",
    "def preprocess_image(image_path, label_path, target_size=(256, 512)):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, target_size)\n",
    "\n",
    "    label = tf.io.read_file(label_path)\n",
    "    label = tf.image.decode_png(label, channels=1)\n",
    "    label = tf.image.resize(label, target_size, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    label = tf.squeeze(label)\n",
    "    return image, tf.cast(label, tf.int32)\n",
    "\n",
    "def augment(image, label):\n",
    "    label = tf.expand_dims(label, axis=-1)\n",
    "    \n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        label = tf.image.flip_left_right(label)\n",
    "\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "\n",
    "    label = tf.squeeze(label, axis=-1)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "def file_paths_generator():\n",
    "    count = 0\n",
    "    for city in sorted(os.listdir(train_images_dir)):\n",
    "        city_images_path = os.path.join(train_images_dir, city)\n",
    "        city_labels_path = os.path.join(train_labels_dir, city)\n",
    "        for image_name in sorted(os.listdir(city_images_path)):\n",
    "            if image_name.endswith('_leftImg8bit.png'):\n",
    "                image_path = os.path.join(city_images_path, image_name)\n",
    "                label_name = image_name.replace('_leftImg8bit.png', '_gtFine_labelIds.png')\n",
    "                label_path = os.path.join(city_labels_path, label_name)\n",
    "                count += 1\n",
    "                yield image_path, label_path\n",
    "    print(f'Total images processed: {count}')\n",
    "\n",
    "def create_dataset():\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator=file_paths_generator,\n",
    "        output_types=(tf.string, tf.string),\n",
    "        output_shapes=(tf.TensorShape([]), tf.TensorShape([])))\n",
    "\n",
    "    dataset = dataset.map(lambda x, y: preprocess_image(x, y))\n",
    "    return dataset.repeat()\n",
    "\n",
    "dataset = create_dataset().batch(8).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "try:\n",
    "    for batch, (images, labels) in enumerate(dataset.take(2)):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(f'Sample Image from Batch {batch+1}')\n",
    "        plt.imshow(images[0].numpy().astype('uint8'))\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(f'Sample Label from Batch {batch+1}')\n",
    "        plt.imshow(labels[0].numpy().squeeze(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print(\"Attempted to access beyond the available data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data_dir = '/home/maith/Desktop/cityscapes'\n",
    "train_labels_dir = os.path.join(data_dir, 'gtFine/train')\n",
    "\n",
    "def analyze_dataset(train_labels_dir):\n",
    "    class_ids = []\n",
    "    for city in sorted(os.listdir(train_labels_dir)):\n",
    "        city_labels_path = os.path.join(train_labels_dir, city)\n",
    "        for label_file in sorted(os.listdir(city_labels_path)):\n",
    "            if label_file.endswith('_labelIds.png'):\n",
    "                label_path = os.path.join(city_labels_path, label_file)\n",
    "                label = tf.io.read_file(label_path)\n",
    "                label = tf.image.decode_png(label, channels=1)\n",
    "                unique_ids = np.unique(label.numpy())\n",
    "                class_ids.extend(unique_ids)\n",
    "    \n",
    "    unique_class_ids = np.unique(class_ids)\n",
    "    return unique_class_ids\n",
    "\n",
    "unique_class_ids = analyze_dataset(train_labels_dir)\n",
    "print(f\"Unique class IDs in the dataset: {unique_class_ids}\")\n",
    "print(f\"Total number of classes: {len(unique_class_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class CustomTileLayer(Layer):\n",
    "    def __init__(self, multiples, **kwargs):\n",
    "        super(CustomTileLayer, self).__init__(**kwargs)\n",
    "        self.multiples = multiples\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        outputs = inputs\n",
    "        for axis, multiple in enumerate(self.multiples):\n",
    "            if multiple > 1:\n",
    "                outputs = tf.expand_dims(outputs, axis)\n",
    "                tile_multiples = [1] * outputs.shape.rank\n",
    "                tile_multiples[axis] = multiple\n",
    "                outputs = tf.tile(outputs, tile_multiples)\n",
    "        output_shape = [input_shape[i] * self.multiples[i] for i in range(len(self.multiples))]\n",
    "        return tf.reshape(outputs, tf.concat([[input_shape[0]], output_shape[1:]], axis=0))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomTileLayer, self).get_config()\n",
    "        config.update({\n",
    "            'multiples': self.multiples,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, UpSampling2D, concatenate, Dropout\n",
    "\n",
    "def conv_block(input_tensor, num_filters, dropout_rate=0.1):\n",
    "    x = Conv2D(num_filters, (3, 3), padding=\"same\")(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    return x\n",
    "\n",
    "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
    "    x = UpSampling2D((2, 2))(input_tensor)\n",
    "    x = concatenate([x, concat_tensor], axis=-1)\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def complex_unet_mobilenetv2(input_shape=(224, 224, 3), num_classes=34):\n",
    "    base_model = MobileNetV2(input_shape=input_shape, include_top=False, alpha=1.0)\n",
    "    base_model.trainable = True\n",
    "\n",
    "    inputs = base_model.input\n",
    "    \n",
    "    f1 = base_model.get_layer(\"block_1_expand_relu\").output\n",
    "    f2 = base_model.get_layer(\"block_3_expand_relu\").output\n",
    "    f3 = base_model.get_layer(\"block_6_expand_relu\").output\n",
    "    f4 = base_model.get_layer(\"block_13_expand_relu\").output\n",
    "    f5 = base_model.get_layer(\"block_16_project\").output\n",
    "    \n",
    "    b = conv_block(f5, 512)\n",
    "\n",
    "    d1 = decoder_block(b, f4, 256)\n",
    "    d2 = decoder_block(d1, f3, 128)\n",
    "    d3 = decoder_block(d2, f2, 64)\n",
    "    d4 = decoder_block(d3, f1, 32)\n",
    "    d5 = UpSampling2D((2, 2))(d4)\n",
    "    \n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(d5)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model = complex_unet_mobilenetv2()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "compile_model(model)\n",
    "\n",
    "print(\"Model compiled successfully with Adam optimizer, Sparse Categorical Crossentropy loss, and accuracy & MeanIoU metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def load_and_preprocess_image(image_path, label_path, target_size=(224, 224)):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, target_size)\n",
    "\n",
    "    label = tf.io.read_file(label_path)\n",
    "    label = tf.image.decode_png(label, channels=1)\n",
    "    label = tf.image.resize(label, target_size, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    label = tf.squeeze(label, axis=-1)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "def prepare_dataset(image_dir, label_dir, batch_size=4):\n",
    "    image_paths = [os.path.join(image_dir, city, f) for city in os.listdir(image_dir) for f in os.listdir(os.path.join(image_dir, city)) if f.endswith('_leftImg8bit.png')]\n",
    "    label_paths = [p.replace('_leftImg8bit.png', '_gtFine_labelIds.png').replace('leftImg8bit', 'gtFine') for p in image_paths]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_paths))\n",
    "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.map(augment, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_image_dir = os.path.join(data_dir, 'leftImg8bit/train')\n",
    "train_label_dir = os.path.join(data_dir, 'gtFine/train')\n",
    "val_image_dir = os.path.join(data_dir, 'leftImg8bit/val')\n",
    "val_label_dir = os.path.join(data_dir, 'gtFine/val')\n",
    "\n",
    "train_dataset = prepare_dataset(train_image_dir, train_label_dir)\n",
    "val_dataset = prepare_dataset(val_image_dir, val_label_dir)\n",
    "\n",
    "print(\"Training and validation datasets are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_dataset_shapes(dataset):\n",
    "#     for images, labels in dataset.take(1):\n",
    "#         print(\"Images shape:\", images.shape)\n",
    "#         print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "# check_dataset_shapes(train_dataset)\n",
    "# check_dataset_shapes(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# def check_model_output(model, input_shape=(1, 256, 512, 3)):\n",
    "#     test_input = tf.random.normal(input_shape)\n",
    "#     test_output = model(test_input)\n",
    "#     print(\"Test output shape:\", test_output.shape)\n",
    "\n",
    "# check_model_output(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "num_train_images = sum([len(files) for r, d, files in os.walk(train_image_dir)])\n",
    "num_val_images = sum([len(files) for r, d, files in os.walk(val_image_dir)])\n",
    "\n",
    "train_steps_per_epoch = num_train_images // 32\n",
    "val_steps_per_epoch = num_val_images // 32\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    os.path.join('/home/maith/Desktop/practical_2/cityscapes/', 'best_model.h5'), \n",
    "    monitor='val_loss', \n",
    "    save_best_only=True, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=val_dataset,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    validation_steps=val_steps_per_epoch,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final_model.h5')\n",
    "print(\"Model saved as final_model.h5\")\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "print(\"Model saved as model.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for images, labels in val_dataset.take(10):\n",
    "    preds = model.predict(images)\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "    true_labels.extend(labels.numpy().flatten())\n",
    "    pred_labels.extend(preds.flatten())\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "pred_labels = np.array(pred_labels)\n",
    "\n",
    "cm = confusion_matrix(true_labels, pred_labels, labels=unique_class_ids)\n",
    "\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=unique_class_ids, yticklabels=unique_class_ids)\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_and_preprocess_image(image_path, label_path, target_size=(256, 512)):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, target_size)\n",
    "\n",
    "    label = tf.io.read_file(label_path)\n",
    "    label = tf.image.decode_png(label, channels=1)\n",
    "    label = tf.image.resize(label, target_size, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    label = tf.squeeze(label, axis=-1)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "# Paths to test images and labels\n",
    "test_images_dir = os.path.join(data_dir, 'leftImg8bit/test')\n",
    "test_labels_dir = os.path.join(data_dir, 'gtFine/test')\n",
    "\n",
    "test_image_paths = [os.path.join(test_images_dir, city, f) for city in os.listdir(test_images_dir) for f in os.listdir(os.path.join(test_images_dir, city)) if f.endswith('_leftImg8bit.png')]\n",
    "test_label_paths = [p.replace('_leftImg8bit.png', '_gtFine_labelIds.png').replace('leftImg8bit', 'gtFine') for p in test_image_paths]\n",
    "\n",
    "index = 5\n",
    "test_image_path = test_image_paths[index]\n",
    "test_label_path = test_label_paths[index]\n",
    "\n",
    "test_image, test_label = load_and_preprocess_image(test_image_path, test_label_path)\n",
    "\n",
    "test_image_batch = tf.expand_dims(test_image, axis=0)\n",
    "predicted_annotation = model.predict(test_image_batch)\n",
    "predicted_annotation = np.argmax(predicted_annotation, axis=-1).squeeze()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Test Image')\n",
    "plt.imshow(test_image.numpy().astype('uint8'))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('True Annotation')\n",
    "plt.imshow(test_label.numpy(), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Predicted Annotation')\n",
    "plt.imshow(predicted_annotation, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function to load and preprocess a single image and its annotation\n",
    "def load_and_preprocess_image(image_path, label_path, target_size=(256, 512)):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, target_size)\n",
    "\n",
    "    label = tf.io.read_file(label_path)\n",
    "    label = tf.image.decode_png(label, channels=1)\n",
    "    label = tf.image.resize(label, target_size, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    label = tf.squeeze(label, axis=-1)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "# Paths to test images and labels\n",
    "test_images_dir = os.path.join(data_dir, 'leftImg8bit/test')\n",
    "test_labels_dir = os.path.join(data_dir, 'gtFine/test')\n",
    "\n",
    "# Get a list of all test images and labels\n",
    "test_image_paths = [os.path.join(test_images_dir, city, f) for city in os.listdir(test_images_dir) for f in os.listdir(os.path.join(test_images_dir, city)) if f.endswith('_leftImg8bit.png')]\n",
    "test_label_paths = [p.replace('_leftImg8bit.png', '_gtFine_labelIds.png').replace('leftImg8bit', 'gtFine') for p in test_image_paths]\n",
    "\n",
    "# Select 5 test images\n",
    "indices = [0, 1, 2, 3, 4]  # Change indices as needed\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, index in enumerate(indices):\n",
    "    # Load and preprocess a single test image and label\n",
    "    test_image_path = test_image_paths[index]\n",
    "    test_label_path = test_label_paths[index]\n",
    "\n",
    "    test_image, test_label = load_and_preprocess_image(test_image_path, test_label_path)\n",
    "\n",
    "    # Predict the annotation for the test image\n",
    "    test_image_batch = tf.expand_dims(test_image, axis=0)  # Add batch dimension\n",
    "    predicted_annotation = model.predict(test_image_batch)\n",
    "    predicted_annotation = np.argmax(predicted_annotation, axis=-1).squeeze()\n",
    "\n",
    "    # Display the test image, true annotation, and predicted annotation\n",
    "    plt.subplot(5, 3, i*3 + 1)\n",
    "    plt.title(f'Test Image {i+1}')\n",
    "    plt.imshow(test_image.numpy().astype('uint8'))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(5, 3, i*3 + 2)\n",
    "    plt.title(f'True Annotation {i+1}')\n",
    "    plt.imshow(test_label.numpy(), cmap='nipy_spectral')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(5, 3, i*3 + 3)\n",
    "    plt.title(f'Predicted Annotation {i+1}')\n",
    "    plt.imshow(predicted_annotation, cmap='nipy_spectral')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "cityscapes_colors = {\n",
    "    0: (128, 64, 128),\n",
    "    1: (244, 35, 232),\n",
    "    2: (70, 70, 70),\n",
    "    3: (102, 102, 156),\n",
    "    4: (190, 153, 153),\n",
    "    5: (153, 153, 153),\n",
    "    6: (250, 170, 30),\n",
    "    7: (220, 220, 0),\n",
    "    8: (107, 142, 35),\n",
    "    9: (152, 251, 152),\n",
    "    10: (70, 130, 180),\n",
    "    11: (220, 20, 60),\n",
    "    12: (255, 0, 0),\n",
    "    13: (0, 0, 142),\n",
    "    14: (0, 0, 70),\n",
    "    15: (0, 60, 100),\n",
    "    16: (0, 80, 100),\n",
    "    17: (0, 0, 230),\n",
    "    18: (119, 11, 32),\n",
    "    19: (0, 0, 0),\n",
    "    20: (0, 0, 0),\n",
    "    21: (0, 0, 0),\n",
    "    22: (0, 0, 0),\n",
    "    23: (0, 0, 70),\n",
    "    24: (0, 60, 100),\n",
    "    25: (0, 80, 100),\n",
    "    26: (0, 0, 230),\n",
    "    27: (119, 11, 32),\n",
    "    28: (70, 70, 70),\n",
    "    29: (102, 102, 156),\n",
    "    30: (190, 153, 153),\n",
    "    31: (153, 153, 153),\n",
    "    32: (250, 170, 30),\n",
    "    33: (220, 220, 0),\n",
    "}\n",
    "\n",
    "def create_color_mask(mask):\n",
    "    color_mask = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
    "    for class_id, color in cityscapes_colors.items():\n",
    "        color_mask[mask == class_id] = color\n",
    "    return color_mask\n",
    "\n",
    "def visualize_predictions(model, test_images, num_images=5):\n",
    "    fig, axes = plt.subplots(num_images, 3, figsize=(20, num_images * 5))\n",
    "    random_indices = random.sample(range(len(test_images)), num_images)\n",
    "\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        test_image_path = test_images[idx]\n",
    "        test_image = tf.image.decode_image(tf.io.read_file(test_image_path))\n",
    "        test_image = tf.image.resize(test_image, (256, 512))\n",
    "        test_image = tf.expand_dims(test_image, axis=0)\n",
    "        \n",
    "        pred_mask = model.predict(test_image)\n",
    "        pred_mask = np.argmax(pred_mask, axis=-1)[0]\n",
    "        pred_color_mask = create_color_mask(pred_mask)\n",
    "\n",
    "        axes[i, 0].imshow(plt.imread(test_image_path))\n",
    "        axes[i, 0].set_title(f\"Test Image {i + 1}\")\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        axes[i, 1].imshow(pred_mask, cmap='gray')\n",
    "        axes[i, 1].set_title(f\"Predicted Annotation {i + 1}\")\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "        axes[i, 2].imshow(pred_color_mask)\n",
    "        axes[i, 2].set_title(f\"Colorized Prediction {i + 1}\")\n",
    "        axes[i, 2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "test_images_dir = '/home/maith/Desktop/practical_2/cityscapes/leftImg8bit/test'\n",
    "test_image_paths = [os.path.join(test_images_dir, city, f) for city in os.listdir(test_images_dir) for f in os.listdir(os.path.join(test_images_dir, city)) if f.endswith('_leftImg8bit.png')]\n",
    "\n",
    "visualize_predictions(model, test_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def load_and_preprocess_image(image_path, label_path, target_size=(256, 512)):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, target_size)\n",
    "\n",
    "    label = tf.io.read_file(label_path)\n",
    "    label = tf.image.decode_png(label, channels=1)\n",
    "    label = tf.image.resize(label, target_size, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    label = tf.squeeze(label, axis=-1)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "def overlay_annotation_on_image(image, annotation, alpha=0.5):\n",
    "    color_annotation = tf.keras.utils.to_categorical(annotation, num_classes=34) \n",
    "    color_annotation = np.argmax(color_annotation, axis=-1)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    colors = np.random.randint(0, 255, size=(34, 3), dtype=int)\n",
    "    \n",
    "    overlay = np.zeros_like(image)\n",
    "    for class_id in range(34):\n",
    "        overlay[color_annotation == class_id] = colors[class_id]\n",
    "    \n",
    "    overlay_image = (alpha * image + (1 - alpha) * overlay).astype(np.uint8)\n",
    "    return overlay_image\n",
    "\n",
    "test_images_dir = os.path.join(data_dir, 'leftImg8bit/test')\n",
    "test_labels_dir = os.path.join(data_dir, 'gtFine/test')\n",
    "\n",
    "test_image_paths = [os.path.join(test_images_dir, city, f) for city in os.listdir(test_images_dir) for f in os.listdir(os.path.join(test_images_dir, city)) if f.endswith('_leftImg8bit.png')]\n",
    "test_label_paths = [p.replace('_leftImg8bit.png', '_gtFine_labelIds.png').replace('leftImg8bit', 'gtFine') for p in test_image_paths]\n",
    "\n",
    "num_images_to_display = 20\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i in range(num_images_to_display):\n",
    "    test_image_path = test_image_paths[i]\n",
    "    test_label_path = test_label_paths[i]\n",
    "\n",
    "    test_image, test_label = load_and_preprocess_image(test_image_path, test_label_path)\n",
    "\n",
    "    test_image_batch = tf.expand_dims(test_image, axis=0)\n",
    "    predicted_annotation = model.predict(test_image_batch)\n",
    "    predicted_annotation = np.argmax(predicted_annotation, axis=-1).squeeze()\n",
    "\n",
    "    overlayed_image = overlay_annotation_on_image(test_image.numpy().astype('uint8'), predicted_annotation)\n",
    "\n",
    "    plt.subplot(num_images_to_display // 5, 5, i + 1)\n",
    "    plt.imshow(overlayed_image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Test Image {i+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to quantized .tflite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "def representative_data_gen():\n",
    "    for input_value, _ in val_dataset.take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('model_quantized.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "print(f\"Model saved as model_quantized.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "custom_objects = {'CustomTileLayer': CustomTileLayer}\n",
    "\n",
    "model = load_model('/home/maith/Desktop/cityscapes/accure_models/final_model.h5', custom_objects=custom_objects)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "def representative_data_gen():\n",
    "    for input_value, _ in val_dataset.take(10):\n",
    "        yield [input_value]\n",
    "\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('model_quantized_no_tile.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "print(f\"Model saved as model_quantized_no_tile.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "custom_objects = {'CustomTileLayer': CustomTileLayer}\n",
    "\n",
    "# Load the original Keras model\n",
    "model = load_model('/home/maith/Desktop/cityscapes/accure_models/best_model.h5', custom_objects=custom_objects)\n",
    "\n",
    "# Convert the model to a TensorFlow Lite model with quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Representative dataset function\n",
    "def representative_data_gen():\n",
    "    for input_value, _ in val_dataset.take(10):\n",
    "        yield [input_value]\n",
    "\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "quantized_tflite_model = converter.convert()\n",
    "\n",
    "# Save the quantized model\n",
    "quantized_model_path = '/home/maith/Desktop/cityscapes/accure_models/model_quantized_3.tflite'\n",
    "with open(quantized_model_path, 'wb') as f:\n",
    "    f.write(quantized_tflite_model)\n",
    "\n",
    "print(f\"Quantized model saved as {quantized_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "custom_objects = {'CustomTileLayer': CustomTileLayer}\n",
    "\n",
    "model = load_model('/home/maith/Desktop/practical_2/cityscapes/final_model.h5', custom_objects=custom_objects)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enet_cityscapes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
