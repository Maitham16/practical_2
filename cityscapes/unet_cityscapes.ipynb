{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_dir = '/home/maith/Desktop/cityscapes'\n",
    "train_images_dir = os.path.join(data_dir, 'leftImg8bit/train')\n",
    "train_labels_dir = os.path.join(data_dir, 'gtFine/train')\n",
    "\n",
    "def preprocess_image(image_path, label_path, target_size=(256, 512)):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, target_size)\n",
    "\n",
    "    label = tf.io.read_file(label_path)\n",
    "    label = tf.image.decode_png(label, channels=1)\n",
    "    label = tf.image.resize(label, target_size, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    label = tf.squeeze(label)\n",
    "    return image, tf.cast(label, tf.int32)\n",
    "\n",
    "def file_paths_generator():\n",
    "    count = 0\n",
    "    for city in sorted(os.listdir(train_images_dir)):\n",
    "        city_images_path = os.path.join(train_images_dir, city)\n",
    "        city_labels_path = os.path.join(train_labels_dir, city)\n",
    "        for image_name in sorted(os.listdir(city_images_path)):\n",
    "            if image_name.endswith('_leftImg8bit.png'):\n",
    "                image_path = os.path.join(city_images_path, image_name)\n",
    "                label_name = image_name.replace('_leftImg8bit.png', '_gtFine_labelIds.png')\n",
    "                label_path = os.path.join(city_labels_path, label_name)\n",
    "                count += 1\n",
    "                yield image_path, label_path\n",
    "    print(f'Total images processed: {count}')\n",
    "\n",
    "def create_dataset():\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator=file_paths_generator,\n",
    "        output_types=(tf.string, tf.string),\n",
    "        output_shapes=((), ()))\n",
    "\n",
    "    dataset = dataset.map(lambda x, y: preprocess_image(x, y))\n",
    "    return dataset.repeat()\n",
    "\n",
    "dataset = create_dataset().batch(8).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "try:\n",
    "    for batch, (images, labels) in enumerate(dataset.take(2)):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(f'Sample Image from Batch {batch+1}')\n",
    "        plt.imshow(images[0].numpy().astype('uint8'))\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(f'Sample Label from Batch {batch+1}')\n",
    "        plt.imshow(labels[0].numpy().squeeze(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print(\"Attempted to access beyond the available data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "data_dir = '/home/maith/Desktop/cityscapes'\n",
    "train_labels_dir = os.path.join(data_dir, 'gtFine/train')\n",
    "\n",
    "def analyze_dataset(train_labels_dir):\n",
    "    class_ids = []\n",
    "    for city in sorted(os.listdir(train_labels_dir)):\n",
    "        city_labels_path = os.path.join(train_labels_dir, city)\n",
    "        for label_file in sorted(os.listdir(city_labels_path)):\n",
    "            if label_file.endswith('_labelIds.png'):\n",
    "                label_path = os.path.join(city_labels_path, label_file)\n",
    "                label = tf.io.read_file(label_path)\n",
    "                label = tf.image.decode_png(label, channels=1)\n",
    "                unique_ids = np.unique(label.numpy())\n",
    "                class_ids.extend(unique_ids)\n",
    "    \n",
    "    unique_class_ids = np.unique(class_ids)\n",
    "    return unique_class_ids\n",
    "\n",
    "# Get unique class IDs\n",
    "unique_class_ids = analyze_dataset(train_labels_dir)\n",
    "print(f\"Unique class IDs in the dataset: {unique_class_ids}\")\n",
    "print(f\"Total number of classes: {len(unique_class_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "\n",
    "def unet_model(input_shape=(256, 512, 3), num_classes=34):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Downsample\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    # Bottleneck\n",
    "    b = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "\n",
    "    # Upsample\n",
    "    u1 = UpSampling2D((2, 2))(b)\n",
    "    concat1 = concatenate([u1, c2])\n",
    "    c3 = Conv2D(128, (3, 3), activation='relu', padding='same')(concat1)\n",
    "    u2 = UpSampling2D((2, 2))(c3)\n",
    "    concat2 = concatenate([u2, c1])\n",
    "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same')(concat2)\n",
    "\n",
    "    # Output\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(c4)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model = unet_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "\n",
    "def compile_model(model):\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    loss = SparseCategoricalCrossentropy()\n",
    "    metrics = ['accuracy', MeanIoU(num_classes=34)]\n",
    "\n",
    "    model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "compile_model(model)\n",
    "\n",
    "print(\"Model compiled successfully with Adam optimizer, Sparse Categorical Crossentropy loss, and accuracy & MeanIoU metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def load_and_preprocess_image(image_path, label_path, target_size=(256, 512)):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, target_size)\n",
    "\n",
    "    label = tf.io.read_file(label_path)\n",
    "    label = tf.image.decode_png(label, channels=1)\n",
    "    label = tf.image.resize(label, target_size, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    label = tf.squeeze(label, axis=-1)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "def prepare_dataset(image_dir, label_dir, batch_size=8):\n",
    "    image_paths = [os.path.join(image_dir, city, f) for city in os.listdir(image_dir) for f in os.listdir(os.path.join(image_dir, city)) if f.endswith('_leftImg8bit.png')]\n",
    "    label_paths = [p.replace('_leftImg8bit.png', '_gtFine_labelIds.png').replace('leftImg8bit', 'gtFine') for p in image_paths]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_paths))\n",
    "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_image_dir = os.path.join(data_dir, 'leftImg8bit/train')\n",
    "train_label_dir = os.path.join(data_dir, 'gtFine/train')\n",
    "val_image_dir = os.path.join(data_dir, 'leftImg8bit/val')\n",
    "val_label_dir = os.path.join(data_dir, 'gtFine/val')\n",
    "\n",
    "train_dataset = prepare_dataset(train_image_dir, train_label_dir)\n",
    "val_dataset = prepare_dataset(val_image_dir, val_label_dir)\n",
    "\n",
    "print(\"Training and validation datasets are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataset_shapes(dataset):\n",
    "    for images, labels in dataset.take(1):\n",
    "        print(\"Images shape:\", images.shape)\n",
    "        print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "check_dataset_shapes(train_dataset)\n",
    "check_dataset_shapes(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "def check_model_output(model, input_shape=(1, 256, 512, 3)):\n",
    "    test_input = tf.random.normal(input_shape)\n",
    "    test_output = model(test_input)\n",
    "    print(\"Test output shape:\", test_output.shape)\n",
    "\n",
    "check_model_output(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    os.path.join('/home/maith/Desktop/cityscapes/', 'best_model.keras'), \n",
    "    monitor='val_loss', \n",
    "    save_best_only=True, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for images, labels in val_dataset:\n",
    "    preds = model.predict(images)\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "    true_labels.extend(labels.numpy().flatten())\n",
    "    pred_labels.extend(preds.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=unique_class_ids, yticklabels=unique_class_ids)\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    dataset = create_dataset().batch(1)\n",
    "    for image, _ in dataset.take(100):\n",
    "        yield [image]\n",
    "\n",
    "model = tf.keras.models.load_model('final_model.keras')\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "try:\n",
    "    tflite_model = converter.convert()\n",
    "    print(\"Model converted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during conversion: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "with open('model_fully_quantized.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model has been successfully converted, fully quantized, and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def convert_to_tflite(model_path, tflite_model_path, val_dataset):\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "    def representative_data_gen():\n",
    "        for input_value, _ in val_dataset.take(10):\n",
    "            yield [input_value]\n",
    "\n",
    "    converter.representative_dataset = representative_data_gen\n",
    "    converter.inference_input_type = tf.uint8\n",
    "    converter.inference_output_type = tf.uint8\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    with open(tflite_model_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    print(f\"Model saved as {tflite_model_path}\")\n",
    "\n",
    "# Path to your saved Keras model\n",
    "model_path = 'final_model.keras'\n",
    "\n",
    "# Desired path for the TFLite model\n",
    "tflite_model_path = 'model_quantized.tflite'\n",
    "\n",
    "# Assuming you have a `val_dataset` for the representative dataset\n",
    "convert_to_tflite(model_path, tflite_model_path, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model\n",
    "tflite_model_path = 'model_quantized_no_tile.tflite'\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get the list of all operations in the model\n",
    "ops = interpreter._get_ops_details()\n",
    "op_names = [op['op_name'] for op in ops]\n",
    "print(op_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_segment(model_segment):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model_segment)\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    ops_details = interpreter._get_ops_details()\n",
    "    op_names = [op['op_name'] for op in ops_details]\n",
    "    if 'TILE' in op_names:\n",
    "        print(\"TILE operation found in this segment!\")\n",
    "    print(\"Operations in this segment:\", op_names)\n",
    "\n",
    "# Usage: create a model segment and pass it to the function\n",
    "segment_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(256, 512, 3)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    # Add more layers incrementally\n",
    "])\n",
    "test_model_segment(segment_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-20 10:33:05.559967: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2024-07-20 10:33:05.565706: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2024-07-20 10:33:05.565743: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ubuntu\n",
      "2024-07-20 10:33:05.565756: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ubuntu\n",
      "2024-07-20 10:33:05.565852: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 535.183.1\n",
      "2024-07-20 10:33:05.565890: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 535.183.1\n",
      "2024-07-20 10:33:05.565902: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 535.183.1\n",
      "2024-07-20 10:33:05.566349: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.5.0\n",
      "Successfully set experimental_new_converter to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-20 10:33:12.865049: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "/home/maith/.pyenv/versions/3.7.12/envs/csenv/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpc8k7q1k9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-20 10:33:19.590533: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-07-20 10:33:19.590677: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2024-07-20 10:33:19.592190: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2419200000 Hz\n",
      "2024-07-20 10:33:19.601066: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.007ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "\n",
      "2024-07-20 10:33:20.180482: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-07-20 10:33:20.180576: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2024-07-20 10:33:20.311126: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\n",
      "  constant_folding: Graph size after: 534 nodes (-357), 860 edges (-323), time = 54.879ms.\n",
      "  constant_folding: Graph size after: 534 nodes (0), 860 edges (0), time = 28.457ms.\n",
      "\n",
      "WARNING:absl:Please consider switching to the new converter by setting experimental_new_converter=True. The old converter (TOCO) is deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion successful!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load your final Keras model\n",
    "model = tf.keras.models.load_model('/home/maith/Desktop/cityscapes/best_model.h5')\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Convert the model to a TensorFlow Lite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Set the converter to use TFLITE_BUILTINS only\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "\n",
    "converter.experimental_new_converter = False\n",
    "print(\"Successfully set experimental_new_converter to False\")\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "print(\"Conversion successful!\")\n",
    "\n",
    "# Save the TensorFlow Lite model\n",
    "with open('model_static.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def test_model_segment(layers):\n",
    "    input_tensor = tf.keras.Input(shape=(256, 512, 3))\n",
    "    x = input_tensor\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    model = tf.keras.Model(inputs=input_tensor, outputs=x)\n",
    "\n",
    "    # Convert the model to TensorFlow Lite\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Check operations\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "    interpreter.allocate_tensors()\n",
    "    ops_set = set(detail['op_name'] for detail in interpreter._get_ops_details())\n",
    "    print(\"Operations:\", ops_set)\n",
    "\n",
    "# Example: Incrementally add layers and test\n",
    "layers_to_test = [\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "]\n",
    "test_model_segment(layers_to_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enet_cityscapes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
